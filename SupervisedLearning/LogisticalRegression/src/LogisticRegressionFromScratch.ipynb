{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistical Regression from Scratch in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function used to reduce the continuous value from \n",
    "# LinearRegression into the (0, 1) \n",
    "# sigmoid(z) = 1 / (1 + e^-z)\n",
    "# also we can't use the cost function from LinearRegression\n",
    "# because it is a non-convex function of weights\n",
    "# so we use J = -y*log(h(x)) - (1-y)log(1-h(x))\n",
    "# where h(x) = sigmoid( theta * x + intercept)\n",
    "# !! y -> real target value(ClassA y = 1, ClassB y = 0)\n",
    "# y = 1 => J = -log(h(x))\n",
    "# y = 0 => J = -log(1-h(x))\n",
    "# we need to maximize the probability by minimizing the loss function\n",
    "# to use now gradient descent for this cost function \n",
    "# theta = theta - alpha * dtheta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class\n",
    "from numpy import ndarray\n",
    "\n",
    "class LogisticReg():\n",
    "    \n",
    "    def __init__(self, learningrate, iterations):\n",
    "        self.learningrate = learningrate\n",
    "        self.iterations = iterations\n",
    "    \n",
    "    def fit(self, X:ndarray, Y:ndarray) -> None:\n",
    "        # partition the no. features and no.entries\n",
    "        self.noentries, self.nofeatures = X.shape\n",
    "        # theta coeff init\n",
    "        self.theta = np.zeros(self.nofeatures)\n",
    "        self.intercept = 0\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        # gradient descent learning\n",
    "        for i in range(self.iterations):\n",
    "            self.update_theta()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def update_theta(self):\n",
    "        # compute the sigmoid\n",
    "        linearvalue = self.X.dot(self.theta) + self.intercept\n",
    "        sig = 1 / (1 + np.exp(-linearvalue))\n",
    "\n",
    "        # compute the gradient \n",
    "        # the final formula of the gradient is \n",
    "        # 1/m * sum(h(x[i]) - y[i]) * x[i][j]\n",
    "        # where j is the index of the theta after which we derivated\n",
    "        # in practice we make use of the matrices and vectors\n",
    "        tmp = (sig - self.Y.T)\n",
    "        tmp = np.reshape(tmp, self.noentries)\n",
    "        dtheta = np.dot(self.X.T, tmp) / self.noentries\n",
    "        dintercept = np.sum(tmp) / self.noentries\n",
    "\n",
    "        #update the weights\n",
    "        self.theta = self.theta - self.learningrate * dtheta\n",
    "        self.intercept = self.intercept - self.learningrate * dintercept\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X: ndarray):\n",
    "        linearvalue = X.dot(self.theta) + self.intercept\n",
    "        Z = 1 / (1 + np.exp(-linearvalue))\n",
    "        Y = np.where(Z > .5, 1, 0)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "Accuracy on test set by our model: 67.578125\n",
      "Accuracy on the model from sklearn: 78.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pcojocar\\Projects\\Personal\\ML\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\pcojocar\\Projects\\Personal\\ML\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# main function\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv('../database/diabetes.csv')\n",
    "    X = data.iloc[: , :-1].values\n",
    "    Y = data.iloc[:, -1:].values\n",
    "\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=1/3, random_state=0)\n",
    "\n",
    "    #model training\n",
    "    selfmademodel = LogisticReg(learningrate=.01, iterations=1000)\n",
    "\n",
    "    selfmademodel.fit(xtrain, ytrain)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(xtrain, ytrain)\n",
    "\n",
    "    #predict the test set\n",
    "    selfmadepred = selfmademodel.predict(xtest)\n",
    "    pred = model.predict(xtest)\n",
    "\n",
    "    #measure performance \n",
    "    self_correctly_classified = 0\n",
    "    correctly_classified = 0\n",
    "\n",
    "    for i in range(np.size(selfmadepred)):\n",
    "        if selfmadepred[i] == ytest[i]:\n",
    "            self_correctly_classified += 1\n",
    "        if pred[i] == ytest[i]:\n",
    "            correctly_classified += 1\n",
    "        \n",
    "    size = np.size(selfmadepred)\n",
    "    print(f\"Accuracy on test set by our model: {(self_correctly_classified / size) * 100}\")\n",
    "    print(f\"Accuracy on the model from sklearn: {(correctly_classified / size) * 100}\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9210526315789473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pcojocar\\AppData\\Local\\Temp\\ipykernel_3756\\1399102110.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray \n",
    "\n",
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, learning_rate = 0.001, iteration=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = iteration\n",
    "        self.theta = None\n",
    "        self.intercept = None \n",
    "\n",
    "    def _sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def fit(self, X:ndarray, y:ndarray):\n",
    "        samplesno, featuresno = X.shape\n",
    "        self.theta = np.zeros(featuresno)\n",
    "        self.intercept = 0\n",
    "\n",
    "        for _ in range(self.iteration):\n",
    "            #compute the linear equivalent of the prediction\n",
    "            linear_pred = np.dot(X, self.theta) + self.intercept\n",
    "            #combine it with the sigmoid value to map it between 0 and 1\n",
    "            predict = LogisticRegression._sigmoid(linear_pred)\n",
    "            #use gradient descent \n",
    "            dtheta = (2/samplesno) * np.dot(X.T, (predict - y))\n",
    "            dintercept = (1/samplesno) * np.sum(predict-y)\n",
    "\n",
    "            self.theta = self.theta - self.learning_rate * dtheta\n",
    "            self.intercept = self.intercept - self.learning_rate * dintercept\n",
    "\n",
    "    def predict(self, X: ndarray):\n",
    "        linear_pred = np.dot(X, self.theta) + self.intercept\n",
    "        pred = LogisticRegression._sigmoid(linear_pred)\n",
    "        label_pred = [0 if y <= .5 else 1 for y in pred]\n",
    "        return label_pred\n",
    "\n",
    "\n",
    "# use the algorithm\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def act():\n",
    "    regr = LogisticRegression()\n",
    "    data = datasets.load_breast_cancer()\n",
    "    X, y = data.data, data.target\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=.2, random_state=1234)\n",
    "\n",
    "    regr.fit(xtrain, ytrain)\n",
    "    pred = regr.predict(xtest)\n",
    "\n",
    "    print(np.sum(pred == ytest)/len(ytest))\n",
    "\n",
    "act()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
